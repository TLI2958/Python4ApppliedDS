{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "    a) Organize information in hierarchy\n",
    "    b) Many similarity measures using this hierarchy in some way\n",
    "        i. path similarity: shortest path between the 2 concepts and similarity measure inversely related to path distance\n",
    "        ii. Lin similarity & lowest common subsumer(LCS): find closest ancestor to both concepts; \n",
    "            LinSim(u,v) = 2 * logP(LCS(u,v))/(log P(u) + log P(u))\n",
    "        iii. Collocations and Distributional Similarity: \n",
    "             Collocation: you know a word by the company it keeps|Firth, 1957\n",
    "             Two words that frequenlty appears in similar contexts are more likeky to be semantically related\n",
    "             e.g., cafe, pizzeria, coffee shop, & restaurant with \"meet\", \"at\"\n",
    "             Distributional Similarity: Context\n",
    "             1. words before, after, within a small window\n",
    "             2. parts of speech of words before, after, in a small window (after a location morality)\n",
    "             3. specific syntactic \n",
    "             4. same sentence, same document, ...\n",
    "             Strength of association between words:\n",
    "             How frequent?\n",
    "             Baseline frequency of ind. words?\n",
    "             - Normalization: Pointwise Mutual Information, PMI(w,c) = log [P(w,c)/P(w)P(c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find appropriate sense of the words\n",
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')\n",
    "horse = wn.synset('horse.n.01')\n",
    "\n",
    "# Find path similarity\n",
    "deer.path_similarity(elk) # .5\n",
    "deer.path_similarity(horse) # .14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7726998936065773"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat') # brown corpus\n",
    "\n",
    "deer.lin_similarity(elk,brown_ic) # .86\n",
    "deer.lin_similarity(horse,brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation & Association Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(text)\n",
    "\n",
    "finder.nbest(bigram_measures.pmi,10)\n",
    "\n",
    "finder also has other useful functions, such as frequency filter\n",
    "\n",
    "i.e., finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What?\n",
    "    \n",
    "    A coarse-level analysis of what is in a text collection\n",
    "    a) Topic: the subject/theme of a discourse\n",
    "    b) Topics are represented as a word distribution\n",
    "    c) In practice,\n",
    "        i. What's known?\n",
    "            1.the text collection or corpus\n",
    "            2.No. of topics\n",
    "        ii. What's unknown?\n",
    "            1. the acutal topics\n",
    "            2. topic distribution for each document\n",
    "    d) Text clustering problem: documents and words are clustered simultaneously\n",
    "    e) Different approaches available\n",
    "        i. Probabilistic Latent Semantic Analysis|PLSA, 99\n",
    "        ii. Latent Dirichlet Allocation |LDA, 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Models and LDA\n",
    "\n",
    "    Pr(text|model)\n",
    "    a) Generative models: \n",
    "    Chest -- ** Generation ** -- Document -- ** inference, estimation ** -- models\n",
    "    i.individual models\n",
    "    ii. mixtual models: how you combine models (topics) to generate such document\n",
    "    \n",
    "    b) LDA: Generative model for a document d\n",
    "    i. choose length of doc d \n",
    "    ii. choose a mixture of topics for doc d\n",
    "    iii. use a topic's multinomial distribution to output words to fill that topic's quota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Topic Modeling in Practice\n",
    "    a) How many topics?\n",
    "        Finding or even guessing the number of topics is hard\n",
    "    b) How to interpret topics?\n",
    "        Topics are just distribution of words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "    a) exploratory text analysis:\n",
    "        What are the documents about?\n",
    "    b) LDA: genism,lda packages\n",
    "    i. Preprocessing:\n",
    "        1. Tokenize, normalize (lowercase)\n",
    "        2. Stop word removal (domain-specific)\n",
    "        3. Stemming\n",
    "    ii. Converting to DTF\n",
    "    iii. Building LDA on top of DTF\n",
    "        1. doc_set\n",
    "        import genism\n",
    "        from genism import corpora,models\n",
    "            \n",
    "        dictionary = corpora.Dictionary(doc_set) # create dictionary\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in doc_set] # DTM, bow = bag-of-word\n",
    "        ldamodel = genism.models.ldamodel.LdaModel(corpus, num_topics = 4, id2word = dictionary, passes = 50) \n",
    "        print(ldamodel.print_topics(num_topics = 4, num_words = 5))\n",
    "    LDA used for extensive corpora\n",
    "    also for feature selections\n",
    "    first step in text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction\n",
    "    \n",
    "    a) Goal: identify and extract fields of interest from free text\n",
    "    i. named entities\n",
    "        [NEWS] People, Place, Dates, Geographic Entities, ... (typically capitalized)\n",
    "        [FINANCE] Money, Companies, ...\n",
    "        [MEDICINE] Drugs, Diseases, Procedures, ...\n",
    "        \n",
    "        HOW?\n",
    "        Techniques to identify all mentions of pre-defined name entities in text:\n",
    "        1.Identify the menton/phrase: Boundary detection\n",
    "        2.Identify type/tag\n",
    "            \n",
    "    ii. relations\n",
    "        What happens to who, when, where\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to identify named entities\n",
    "\n",
    "    Depends on kinds of entities that need to be identified\n",
    "        a) for well-formatted fields like data, phone numbers: Regular Expressions\n",
    "        b) for other fields: Typicallly a ML approach\n",
    "    \n",
    "    Standard NER task in NLP processing: four-class model\n",
    "        PER\n",
    "        ORG\n",
    "        LOC/GPE\n",
    "        Other/Outside (any other class)\n",
    "    * for NEWS, nltk has an embedded function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Extraction\n",
    "    Co-reference Resolution\n",
    "    i. Disambiguate mentions and group mentions together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
