{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "    a) Organize information in hierarchy\n",
    "    b) Many similarity measures using this hierarchy in some way\n",
    "        i. path similarity: shortest path between the 2 concepts and similarity measure inversely related to path distance\n",
    "        ii. Lin similarity & lowest common subsumer(LCS): find closest ancestor to both concepts; \n",
    "            LinSim(u,v) = 2 * logP(LCS(u,v))/(log P(u) + log P(u))\n",
    "        iii. Collocations and Distributional Similarity: \n",
    "             Collocation: you know a word by the company it keeps|Firth, 1957\n",
    "             Two words that frequenlty appears in similar contexts are more likeky to be semantically related\n",
    "             e.g., cafe, pizzeria, coffee shop, & restaurant with \"meet\", \"at\"\n",
    "             Distributional Similarity: Context\n",
    "             1. words before, after, within a small window\n",
    "             2. parts of speech of words before, after, in a small window (after a location morality)\n",
    "             3. specific syntactic \n",
    "             4. same sentence, same document, ...\n",
    "             Strength of association between words:\n",
    "             How frequent?\n",
    "             Baseline frequency of ind. words?\n",
    "             - Normalization: Pointwise Mutual Information, PMI(w,c) = log [P(w,c)/P(w)P(c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find appropriate sense of the words\n",
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')\n",
    "horse = wn.synset('horse.n.01')\n",
    "\n",
    "# Find path similarity\n",
    "deer.path_similarity(elk) # .5\n",
    "deer.path_similarity(horse) # .14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7726998936065773"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat') # brown corpus\n",
    "\n",
    "deer.lin_similarity(elk,brown_ic) # .86\n",
    "deer.lin_similarity(horse,brown_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation & Association Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(text)\n",
    "\n",
    "finder.nbest(bigram_measures.pmi,10)\n",
    "\n",
    "finder also has other useful functions, such as frequency filter\n",
    "\n",
    "i.e., finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What?\n",
    "    \n",
    "    A coarse-level analysis of what is in a text collection\n",
    "    a) Topic: the subject/theme of a discourse\n",
    "    b) Topics are represented as a word distribution\n",
    "    c) In practice,\n",
    "        i. What's known?\n",
    "            1.the text collection or corpus\n",
    "            2.No. of topics\n",
    "        ii. What's unknown?\n",
    "            1. the acutal topics\n",
    "            2. topic distribution for each document\n",
    "    d) Text clustering problem: documents and words are clustered simultaneously\n",
    "    e) Different approaches available\n",
    "        i. Probabilistic Latent Semantic Analysis|PLSA, 99\n",
    "        ii. Latent Dirichlet Allocation |LDA, 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Models and LDA\n",
    "\n",
    "    Pr(text|model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
